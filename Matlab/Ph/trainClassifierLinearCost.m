function [trainedClassifier, validationAccuracy,validationPredictions] = trainClassifierLinearCost(trainingData,costM)
% [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% Returns a trained classifier and its accuracy. This code recreates the
% classification model trained in Classification Learner app. Use the
% generated code to automate training the same model with new data, or to
% learn how to programmatically train models.
%
%  Input:
%      trainingData: A table containing the same predictor and response
%       columns as those imported into the app.
%
%  Output:
%      trainedClassifier: A struct containing the trained classifier. The
%       struct contains various fields with information about the trained
%       classifier.
%
%      trainedClassifier.predictFcn: A function to make predictions on new
%       data.
%
%      validationAccuracy: A double containing the accuracy in percent. In
%       the app, the History list displays this overall accuracy score for
%       each model.
%
% Use the code to train the model with new data. To retrain your
% classifier, call the function from the command line with your original
% data or new data as the input argument trainingData.
%
% For example, to retrain a classifier trained with the original data set
% T, enter:
%   [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
% To make predictions with the returned 'trainedClassifier' on new data T2,
% use
%   yfit = trainedClassifier.predictFcn(T2)
%
% T2 must be a table containing at least the same predictor columns as used
% during training. For details, enter:
%   trainedClassifier.HowToPredict

% Auto-generated by MATLAB on 19-Aug-2020 16:34:07


% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'Sand','Clay','Silt','scE_400','scE_4085','scE_417','scE_4255','scE_434','scE_4425','scE_451','scE_4595','scE_468','scE_4765','scE_485','scE_4935','scE_502','scE_5105','scE_519','scE_5275','scE_536','scE_5445','scE_553','scE_5615','scE_570','scE_5785','scE_587','scE_5955','scE_604','scE_6125','scE_621','scE_6295','scE_638','scE_6465','scE_655','scE_6635','scE_672','scE_6805','scE_689','scE_6975','scE_706','scE_7145','scE_723','scE_7315','scE_740','scE_7485','scE_757','scE_7655','scE_774','scE_7825','scE_791','scE_7995','scE_808','scE_8165','scE_825','scE_8335','scE_842','scE_8505','scE_859','scE_8675','scE_876','scE_8845','scE_893','scE_9015','scE_910','scE_9185','scE_927','scE_9355','scE_944','scE_9525','scE_961','scE_9695','scE_978','scE_9865','scE_995','scE_10035','scE_1012','scE_10205','scE_1029','scE_10375','scE_1046','scE_10545','scE_1063','scE_10715','scE_1080','scE_10885','scE_1097','scE_11055','scE_1114','scE_11225','scE_1131','scE_11395','scE_1148','scE_11565','scE_1165','scE_11735','scE_1182','scE_11905','scE_1199','scE_12075','scE_1216','scE_12245','scE_1233','scE_12415','scE_1250','scE_12585','scE_1267','scE_12755','scE_1284','scE_12925','scE_1301','scE_13095','scE_1318','scE_13265','scE_1335','scE_13435','scE_1352','scE_13605','scE_1369','scE_13775','scE_1386','scE_13945','scE_1403','scE_14115','scE_1420','scE_14285','scE_1437','scE_14455','scE_1454','scE_14625','scE_1471','scE_14795','scE_1488','scE_14965','scE_1505','scE_15135','scE_1522','scE_15305','scE_1539','scE_15475','scE_1556','scE_15645','scE_1573','scE_15815','scE_1590','scE_15985','scE_1607','scE_16155','scE_1624','scE_16325','scE_1641','scE_16495','scE_1658','scE_16665','scE_1675','scE_16835','scE_1692','scE_17005','scE_1709','scE_17175','scE_1726','scE_17345','scE_1743','scE_17515','scE_1760','scE_17685','scE_1777','scE_17855','scE_1794','scE_18025','scE_1811','scE_18195','scE_1828','scE_18365','scE_1845','scE_18535','scE_1862','scE_18705','scE_1879','scE_18875','scE_1896','scE_19045','scE_1913','scE_19215','scE_1930','scE_19385','scE_1947','scE_19555','scE_1964','scE_19725','scE_1981','scE_19895','scE_1998','scE_20065','scE_2015','scE_20235','scE_2032','scE_20405','scE_2049','scE_20575','scE_2066','scE_20745','scE_2083','scE_20915','scE_2100','scE_21085','scE_2117','scE_21255','scE_2134','scE_21425','scE_2151','scE_21595','scE_2168','scE_21765','scE_2185','scE_21935','scE_2202','scE_22105','scE_2219','scE_22275','scE_2236','scE_22445','scE_2253','scE_22615','scE_2270','scE_22785','scE_2287','scE_22955','scE_2304','scE_23125','scE_2321','scE_23295','scE_2338','scE_23465','scE_2355','scE_23635','scE_2372','scE_23805','scE_2389','scE_23975','scE_2406','scE_24145','scE_2423','scE_24315','scE_2440','scE_24485','scE_2457','scE_24655','scE_2474','scE_24825','scE_2491','d1_400','d1_4085','d1_417','d1_4255','d1_434'};
predictors = inputTable(:, predictorNames);
response = inputTable.Et_pH_3Cat;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
template = templateSVM(...
    'KernelFunction', 'linear', ...
    'PolynomialOrder', [], ...
    'KernelScale', 'auto', ...
    'BoxConstraint', 1, ...
    'Standardize', true);
classificationSVM = fitcecoc(...
    predictors, ...
    response, ...
    'Learners', template, ...
    'Coding', 'onevsone', ...
    'Cost', costM, ...
    'ClassNames', categorical({'Sin_Correccion'; 'Correccion_Acidez'; 'Correccion_Alcalinidad'}));

% Create the result struct with predict function
predictorExtractionFcn = @(t) t(:, predictorNames);
svmPredictFcn = @(x) predict(classificationSVM, x);
trainedClassifier.predictFcn = @(x) svmPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.RequiredVariables = {'Sand','Clay','Silt','scE_400','scE_4085','scE_417','scE_4255','scE_434','scE_4425','scE_451','scE_4595','scE_468','scE_4765','scE_485','scE_4935','scE_502','scE_5105','scE_519','scE_5275','scE_536','scE_5445','scE_553','scE_5615','scE_570','scE_5785','scE_587','scE_5955','scE_604','scE_6125','scE_621','scE_6295','scE_638','scE_6465','scE_655','scE_6635','scE_672','scE_6805','scE_689','scE_6975','scE_706','scE_7145','scE_723','scE_7315','scE_740','scE_7485','scE_757','scE_7655','scE_774','scE_7825','scE_791','scE_7995','scE_808','scE_8165','scE_825','scE_8335','scE_842','scE_8505','scE_859','scE_8675','scE_876','scE_8845','scE_893','scE_9015','scE_910','scE_9185','scE_927','scE_9355','scE_944','scE_9525','scE_961','scE_9695','scE_978','scE_9865','scE_995','scE_10035','scE_1012','scE_10205','scE_1029','scE_10375','scE_1046','scE_10545','scE_1063','scE_10715','scE_1080','scE_10885','scE_1097','scE_11055','scE_1114','scE_11225','scE_1131','scE_11395','scE_1148','scE_11565','scE_1165','scE_11735','scE_1182','scE_11905','scE_1199','scE_12075','scE_1216','scE_12245','scE_1233','scE_12415','scE_1250','scE_12585','scE_1267','scE_12755','scE_1284','scE_12925','scE_1301','scE_13095','scE_1318','scE_13265','scE_1335','scE_13435','scE_1352','scE_13605','scE_1369','scE_13775','scE_1386','scE_13945','scE_1403','scE_14115','scE_1420','scE_14285','scE_1437','scE_14455','scE_1454','scE_14625','scE_1471','scE_14795','scE_1488','scE_14965','scE_1505','scE_15135','scE_1522','scE_15305','scE_1539','scE_15475','scE_1556','scE_15645','scE_1573','scE_15815','scE_1590','scE_15985','scE_1607','scE_16155','scE_1624','scE_16325','scE_1641','scE_16495','scE_1658','scE_16665','scE_1675','scE_16835','scE_1692','scE_17005','scE_1709','scE_17175','scE_1726','scE_17345','scE_1743','scE_17515','scE_1760','scE_17685','scE_1777','scE_17855','scE_1794','scE_18025','scE_1811','scE_18195','scE_1828','scE_18365','scE_1845','scE_18535','scE_1862','scE_18705','scE_1879','scE_18875','scE_1896','scE_19045','scE_1913','scE_19215','scE_1930','scE_19385','scE_1947','scE_19555','scE_1964','scE_19725','scE_1981','scE_19895','scE_1998','scE_20065','scE_2015','scE_20235','scE_2032','scE_20405','scE_2049','scE_20575','scE_2066','scE_20745','scE_2083','scE_20915','scE_2100','scE_21085','scE_2117','scE_21255','scE_2134','scE_21425','scE_2151','scE_21595','scE_2168','scE_21765','scE_2185','scE_21935','scE_2202','scE_22105','scE_2219','scE_22275','scE_2236','scE_22445','scE_2253','scE_22615','scE_2270','scE_22785','scE_2287','scE_22955','scE_2304','scE_23125','scE_2321','scE_23295','scE_2338','scE_23465','scE_2355','scE_23635','scE_2372','scE_23805','scE_2389','scE_23975','scE_2406','scE_24145','scE_2423','scE_24315','scE_2440','scE_24485','scE_2457','scE_24655','scE_2474','scE_24825','scE_2491','d1_400','d1_4085','d1_417','d1_4255','d1_434'};
trainedClassifier.ClassificationSVM = classificationSVM;
trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2020a.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'Sand','Clay','Silt','scE_400','scE_4085','scE_417','scE_4255','scE_434','scE_4425','scE_451','scE_4595','scE_468','scE_4765','scE_485','scE_4935','scE_502','scE_5105','scE_519','scE_5275','scE_536','scE_5445','scE_553','scE_5615','scE_570','scE_5785','scE_587','scE_5955','scE_604','scE_6125','scE_621','scE_6295','scE_638','scE_6465','scE_655','scE_6635','scE_672','scE_6805','scE_689','scE_6975','scE_706','scE_7145','scE_723','scE_7315','scE_740','scE_7485','scE_757','scE_7655','scE_774','scE_7825','scE_791','scE_7995','scE_808','scE_8165','scE_825','scE_8335','scE_842','scE_8505','scE_859','scE_8675','scE_876','scE_8845','scE_893','scE_9015','scE_910','scE_9185','scE_927','scE_9355','scE_944','scE_9525','scE_961','scE_9695','scE_978','scE_9865','scE_995','scE_10035','scE_1012','scE_10205','scE_1029','scE_10375','scE_1046','scE_10545','scE_1063','scE_10715','scE_1080','scE_10885','scE_1097','scE_11055','scE_1114','scE_11225','scE_1131','scE_11395','scE_1148','scE_11565','scE_1165','scE_11735','scE_1182','scE_11905','scE_1199','scE_12075','scE_1216','scE_12245','scE_1233','scE_12415','scE_1250','scE_12585','scE_1267','scE_12755','scE_1284','scE_12925','scE_1301','scE_13095','scE_1318','scE_13265','scE_1335','scE_13435','scE_1352','scE_13605','scE_1369','scE_13775','scE_1386','scE_13945','scE_1403','scE_14115','scE_1420','scE_14285','scE_1437','scE_14455','scE_1454','scE_14625','scE_1471','scE_14795','scE_1488','scE_14965','scE_1505','scE_15135','scE_1522','scE_15305','scE_1539','scE_15475','scE_1556','scE_15645','scE_1573','scE_15815','scE_1590','scE_15985','scE_1607','scE_16155','scE_1624','scE_16325','scE_1641','scE_16495','scE_1658','scE_16665','scE_1675','scE_16835','scE_1692','scE_17005','scE_1709','scE_17175','scE_1726','scE_17345','scE_1743','scE_17515','scE_1760','scE_17685','scE_1777','scE_17855','scE_1794','scE_18025','scE_1811','scE_18195','scE_1828','scE_18365','scE_1845','scE_18535','scE_1862','scE_18705','scE_1879','scE_18875','scE_1896','scE_19045','scE_1913','scE_19215','scE_1930','scE_19385','scE_1947','scE_19555','scE_1964','scE_19725','scE_1981','scE_19895','scE_1998','scE_20065','scE_2015','scE_20235','scE_2032','scE_20405','scE_2049','scE_20575','scE_2066','scE_20745','scE_2083','scE_20915','scE_2100','scE_21085','scE_2117','scE_21255','scE_2134','scE_21425','scE_2151','scE_21595','scE_2168','scE_21765','scE_2185','scE_21935','scE_2202','scE_22105','scE_2219','scE_22275','scE_2236','scE_22445','scE_2253','scE_22615','scE_2270','scE_22785','scE_2287','scE_22955','scE_2304','scE_23125','scE_2321','scE_23295','scE_2338','scE_23465','scE_2355','scE_23635','scE_2372','scE_23805','scE_2389','scE_23975','scE_2406','scE_24145','scE_2423','scE_24315','scE_2440','scE_24485','scE_2457','scE_24655','scE_2474','scE_24825','scE_2491','d1_400','d1_4085','d1_417','d1_4255','d1_434'};
predictors = inputTable(:, predictorNames);
response = inputTable.Et_pH_3Cat;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Perform cross-validation
partitionedModel = crossval(trainedClassifier.ClassificationSVM, 'KFold', 5);

% Compute validation predictions
[validationPredictions, validationScores] = kfoldPredict(partitionedModel);

% Compute validation accuracy
validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');
